{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pNIO2JKrHJiW"
   },
   "source": [
    "# Analyzing Livestream Data from Twitch \n",
    "\n",
    "In order to connect to Twitch chat, one should use Python Sockets to connect via IRC. To stream messages from the Twitch IRC, you need to get a token for authentication. To do that, you need a Twitch account. I've created a development team account. Now, you need to go to the following [link]( https://twitchapps.com/tmi/) while signed in to obtain an Auth token for your Twitch account. You click authorize and keep that token handy for later. \n",
    "\n",
    "Below, we define a few constants for obtaining Twitch data. Channel responds to the streamer's name and can be the name of any channel one is interested in. Currently, this is set up for one chat stream at a time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important Constants for Connection to Twitch Channel Feed\n",
    "server = 'irc.chat.twitch.tv'\n",
    "port = 6667\n",
    "nickname = 'acn_development_team' # '<YOUR_USERNAME>'\n",
    "token = \n",
    "channel = '#giantbomb' # The hashtag before the username here is SUPER important. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connecting to Twitch using Sockets\n",
    "\n",
    "To establish a connection to Twitch IRC, we'll use Python's socket library. So we first instantiate a socket:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "sock = socket.socket()\n",
    "sock.settimeout(120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to connect the socket to Twitch by calling connect() with the server and port previously defined. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sock.connect((server,port))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once connected, we need to send our token and nickname for authentication as well as the channel we care about. These are sent via encoded strings, so we need to send them in this very specific format: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where 'utf-8' encoding encodes the string into bytes which allows it to be sent over the socket. \n",
    "sock.send(f\"PASS {token}\\n\".encode('utf-8')) # carries token \n",
    "sock.send(f\"NICK {nickname}\\n\".encode('utf-8')) # carries nickname\n",
    "sock.send(f\"JOIN {channel}\\n\".encode('utf-8')) # carries channel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recieving Channel Messages \n",
    "# Example: Getting a single response. \n",
    "# This is commented out because it's just for testing out one example. \n",
    "\n",
    "# resp = sock.recv(2048).decode('utf-8') #2048 is the buffer size in bytes, amount of data to recieve. \n",
    "# resp\n",
    "\n",
    "\n",
    "# sock.close() # Use this to open/close the socket. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Currently, the socket gets responses but we need to check for new messages and log messages as they come in. \n",
    "# The proper way to do this is to set up a logger that writes messages to a file and a loop that will check for new messages as the socket's open. \n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG,\n",
    "                    format='%(asctime)s — %(message)s',\n",
    "                    datefmt='%Y-%m-%d_%H:%M:%S',\n",
    "                    handlers=[logging.FileHandler('chat2.log', encoding='utf-8')])\n",
    "\n",
    "# Debug means all levels of logging can be written to the file. \n",
    "# Format is how we want the line to look, which will be time/message. Date format is how the time of the format is recorded. \n",
    "# FileHandler to handlers is passed which creates a file in the directory and logs info to chat.log. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logging.info(resp) # We can open our repository and notice chat.log is updated now! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuously checking for new messages in a loop. When connected to IRC, we want to make sure to send \"PONG\" if server sends \"PING.\"\n",
    "# Also want to parse emojis so they can be written to a file. There's an emoji library that maps emoji to word meaning. \n",
    "\n",
    "from emoji import demojize\n",
    "\n",
    "while True:\n",
    "    resp = sock.recv(2048).decode('utf-8')\n",
    "\n",
    "    if resp.startswith('PING'):\n",
    "        sock.send(\"PONG\\n\".encode('utf-8'))\n",
    "    \n",
    "    elif len(resp) > 0:\n",
    "        logging.info(demojize(resp))\n",
    "        \n",
    "# This will keep running until you stop it. \n",
    "# To see the messages in real-time open a new terminal, navigate to the log's location, and run tail -f chat.log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sock.close() # Use this to open/close the socket. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing the Chat Logs from Twitch\n",
    "In this case, we want to parse the chat log into a Pandas DataFrame to prepare for analysis. This means that we want to have the username, message, date, and time information (but really I care only about the message). Because we need to get data from each line, we need to be strategic about how we read info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "def get_chat_dataframe(file):\n",
    "    data = []\n",
    "\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        lines = f.read().split('\\n')\n",
    "        \n",
    "        for line in lines:\n",
    "            try:\n",
    "                time_logged = line.split('—')[0].strip()\n",
    "                time_logged = datetime.strptime(time_logged, '%Y-%m-%d_%H:%M:%S')\n",
    "\n",
    "                username_message = line.split('—')[1:]\n",
    "                username_message = '—'.join(username_message).strip()\n",
    "\n",
    "                username, channel, message = re.search(\n",
    "                    ':(.*)\\!.*@.*\\.tmi\\.twitch\\.tv PRIVMSG #(.*) :(.*)', username_message\n",
    "                ).groups()\n",
    "\n",
    "                d = {\n",
    "                    'dt': time_logged,\n",
    "                    'channel': channel,\n",
    "                    'username': username,\n",
    "                    'message': message\n",
    "                }\n",
    "\n",
    "                data.append(d)\n",
    "            \n",
    "            except Exception:\n",
    "                pass\n",
    "            \n",
    "    return pd.DataFrame().from_records(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_chat_dataframe('chat.log')\n",
    "\n",
    "df.set_index('dt', inplace=True)\n",
    "\n",
    "print(df.shape)\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, employing my own custom data processing. I'm personally only interested in the messages feature! \n",
    "df['message'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Dataset, Visualizing Datset, and Building LDA Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re, nltk, spacy, gensim\n",
    "\n",
    "#TextHero\n",
    "# !pip install texthero -U   \n",
    "import texthero as hero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pass a custom pipeline as argument to clean\n",
    "\n",
    "from texthero import preprocessing\n",
    "# https://pypi.org/project/texthero/\n",
    "# Texthero is a really good library built off of Pandas and it allows for those with minimal knowledge of the NLP space to utilize powerful tools. \n",
    "\n",
    "custom_pipeline = [preprocessing.lowercase, # much easier than previous solution which was to df.remove. \n",
    "                   preprocessing.remove_punctuation,\n",
    "                   preprocessing.remove_urls]\n",
    "data = hero.clean(df['message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom STOPWORDS removal\n",
    "from texthero import stopwords\n",
    "default_stopwords = stopwords.DEFAULT\n",
    "custom_stopwords = default_stopwords.union(set([\"twitch\",\"make\", \"use\", \"thank\", \"content\", \"good\", \"use\",\n",
    "                                                \"think\", \"need\", \"harrisheller\", \"like\", \"stream\",\n",
    "                                               \"kekw\",\"catjam\",\"tim\",\"timthetatman\",\"tatkevinh\",\n",
    "                                               \"wipz\",\"docspin\",\"pog\",\"tatlove\",\"lol\",\"lul\",\"omegalul\",\n",
    "                                               \"biblethump\",\"clap\",\"tathypers\",\"pepeja\",\"kappa\",\"tattopd\",\"ppsmoke\",\n",
    "                                               \"pepelaugh\",\"gopackgo\",\"gachihyper\",\"tatkevinh\", \"wipz\",\n",
    "                                               \"pausechamp\",\"yep\",\"lmao\",\"jack\",\"lulw\",\"monkaw\",\"kreygasm\",\n",
    "                                               \"pepega\",\"peped\",\"foxsalt\",\"pogchamp\",\"xqcn\",\"get\",\"back\",\n",
    "                                               \"tattuff\",\"tatfat\",\"tatpumpkin\",\"lmao\",\"sadge\",\"sippy\",\n",
    "                                               \"pogu\",\"poggers\",\"consolecd\",\"widepeepohappy\",\"pogu\",\"tategg2\",\n",
    "                                               \"modcheck\",\"timmy\",\"tathmm\",\"tats\",\"got\",\"com\",\"babyrage\",\n",
    "                                               \"xqcp\",\"tatw\",\"pokiw\",\"know\", \"thats\",\"pepocd\",\"tatafk\",\n",
    "                                               \"4weird\",\"tatkkevin\", \"tatblanket\",\"tatglam\",\"tategg1\",\"wutface\",\n",
    "                                               \"blobdance\", \"kapp\",\"tatbruh\",\"kappapride\",\"facebaby\",\"xqc\",\n",
    "                                               \"xqcm\",\"bora\",\"hyperclap\",\"tatlit\",\"5head\",\"gachibass\", \"go\", \"ur\",\n",
    "                                                \"yes\",\"going\",\"would\",\"im\",\"oh\",\"dez\",\"taty\",\"tk\",\"u\",\"sg\", \"dont\",\n",
    "                                                \"hey\",\"hf\",\"look\"\n",
    "                                               ])) ## Add as per requirement\n",
    "# data = hero.remove_stopwords(data, default_stopwords)\n",
    "data = hero.remove_stopwords(data, custom_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hero.visualization.wordcloud(data, font_path = None, width = 400, height = 200, max_words=200, \n",
    "                             mask=None, contour_width=0, \n",
    "                             contour_color='PAPAYAWHIP', background_color='WHITE', \n",
    "                             relative_scaling='auto', colormap=None, return_figure=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "print(data_words[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize\n",
    "# Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, \n",
    "# normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma.\n",
    "# tl;dr put the verbs in their \"stem\" form. \n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append(\" \".join([token.lemma_ if token.lemma_ not in ['-PRON-'] else '' for token in doc if token.pos_ in allowed_postags]))\n",
    "    return texts_out\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only Noun, Adjective, Verb, Adverb\n",
    "data_lemmatized = lemmatization(data_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(data_lemmatized[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word-Document Matrix\n",
    "# This converts a collection of text documents to a matrix of token counts. \n",
    "# A document-term matrix or term-document matrix is a mathematical matrix that describes the frequency of terms that occur in a collection of documents. \n",
    "# In a document-term matrix, rows correspond to documents in the collection and columns correspond to terms. \n",
    "\n",
    "vectorizer = CountVectorizer(analyzer='word',       \n",
    "                             min_df=10,                        # minimum number occurences of a word required\n",
    "                             stop_words='english',             # remove stop words\n",
    "                             lowercase=True,                   # convert all words to lowercase\n",
    "                             token_pattern='[a-zA-Z0-9]{3,}')  # num of characters > 3\n",
    "                            \n",
    "\n",
    "data_vectorized = vectorizer.fit_transform(data_lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sklearn\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from pprint import pprint\n",
    "\n",
    "# Plotting tools\n",
    "# !pip install -U pyLDAvis\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model with Sklearn\n",
    "\n",
    "lda_model = LatentDirichletAllocation(n_components=15,               # Number of topics\n",
    "                                      max_iter=10,                   # Max learning iterations\n",
    "                                      learning_method='online',   \n",
    "                                      random_state=100,              # Random state\n",
    "                                      batch_size=128)                # n docs in each learning iter\n",
    "                                            \n",
    "                                      \n",
    "lda_output = lda_model.fit_transform(data_vectorized)\n",
    "\n",
    "print(lda_model)  # Model attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate model quality\n",
    "\n",
    "# Log Likelihood: Higher the better\n",
    "print(\"Log Likelihood: \", lda_model.score(data_vectorized))\n",
    "\n",
    "# Perplexity: Lower the better. Perplexity = exp((-1) * log-likelihood per word)\n",
    "print(\"Perplexity: \", lda_model.perplexity(data_vectorized))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search for Best LDA Model\n",
    "# Grid search is a tuning technique that attempts to compute the optimum values of hyperparameters. \n",
    "# It is an exhaustive search that is performed on a the specific parameter values of a model. The model is also known as an estimator.\n",
    "# This is computationally expensive and usually takes time... \n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define Search Param\n",
    "params = {'n_components': [10, 15, 20, 25, 30], 'learning_decay': [.5, .7, .9]}\n",
    "\n",
    "# Model\n",
    "lda = LatentDirichletAllocation()\n",
    "\n",
    "# Grid Search\n",
    "model = GridSearchCV(lda, param_grid=params)\n",
    "\n",
    "# Perform Grid Search\n",
    "model.fit(data_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Model\n",
    "best_lda_model = model.best_estimator_\n",
    "\n",
    "# Model Parameters\n",
    "print(\"Best Model's Params: \", model.best_params_)\n",
    "\n",
    "# Log Likelihood Score\n",
    "print(\"Best Log Likelihood Score: \", model.best_score_)\n",
    "\n",
    "# Perplexity\n",
    "print(\"Model Perplexity: \", best_lda_model.perplexity(data_vectorized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize with pyLDAvis\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "panel = pyLDAvis.sklearn.prepare(best_lda_model, data_vectorized, vectorizer, mds='tsne')\n",
    "panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reflections:\n",
    "# This exercise shows how you can analyze a massive corpus of text and split data into \"topics\" which contain certain words that appear \n",
    "# frequently amongst those topics. In this case, we have 10 topics and 10 words that reflect those topics. What this data can tell us is \n",
    "# how important and how frequent certain words are in the corpus - what themes/etc. may emerge. \n",
    "\n",
    "# Show top n keywords for each topic\n",
    "\n",
    "def show_topics(vectorizer=vectorizer, lda_model=lda_model, n_words=20):\n",
    "    keywords = np.array(vectorizer.get_feature_names())\n",
    "    topic_keywords = []\n",
    "    for topic_weights in lda_model.components_:\n",
    "        top_keyword_locs = (-topic_weights).argsort()[:n_words]\n",
    "        topic_keywords.append(keywords.take(top_keyword_locs))\n",
    "    return topic_keywords\n",
    "\n",
    "topic_keywords = show_topics(vectorizer=vectorizer, lda_model=best_lda_model, n_words=10)        \n",
    "\n",
    "# Topic - Keywords Dataframe\n",
    "df_topic_keywords = pd.DataFrame(topic_keywords)\n",
    "df_topic_keywords.columns = ['Word '+str(i) for i in range(df_topic_keywords.shape[1])]\n",
    "df_topic_keywords.index = ['Topic '+str(i) for i in range(df_topic_keywords.shape[0])]\n",
    "df_topic_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, focusing on Semantic Text Similarity.\n",
    "# First, we're adding our import statements. \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import os \n",
    "import re\n",
    "import operator\n",
    "import pickle\n",
    "import nltk \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, doing Word Tokenization\n",
    "# Tokenization is hwen each entry in the data is broken down into a set of words.\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "df['Word tokenize']= [word_tokenize(entry) for entry in df.message]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WordNetLemmatizer requires Pos tags to understand if the word is noun or verb or adjective etc. By default it is set to Noun\n",
    "def wordLemmatizer(data):\n",
    "    tag_map = defaultdict(lambda : wn.NOUN)\n",
    "    tag_map['J'] = wn.ADJ\n",
    "    tag_map['V'] = wn.VERB\n",
    "    tag_map['R'] = wn.ADV\n",
    "    file_clean_k =pd.DataFrame()\n",
    "    for index,entry in enumerate(data):\n",
    "        \n",
    "        # Declaring Empty List to store the words that follow the rules for this step\n",
    "        Final_words = []\n",
    "        # Initializing WordNetLemmatizer()\n",
    "        word_Lemmatized = WordNetLemmatizer()\n",
    "        # pos_tag function below will provide the 'tag' i.e if the word is Noun(N) or Verb(V) or something else.\n",
    "        for word, tag in pos_tag(entry):\n",
    "            # Below condition is to check for Stop words and consider only alphabets - I turned this part off for now. \n",
    "#             if len(word)>1 and word not in stopwords.words('english') and word.isalpha():\n",
    "                word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])\n",
    "                Final_words.append(word_Final)\n",
    "            # The final processed set of words for each iteration will be stored in 'text_final'\n",
    "                file_clean_k.loc[index,'Keyword_final'] = str(Final_words)\n",
    "                file_clean_k.loc[index,'Keyword_final'] = str(Final_words)\n",
    "                #file_clean_k=file_clean_k.replace(to_replace =\"\\[.\", value = '', regex = True)\n",
    "                #file_clean_k=file_clean_k.replace(to_replace =\"'\", value = '', regex = True)\n",
    "                #file_clean_k=file_clean_k.replace(to_replace =\" \", value = '', regex = True)\n",
    "                #file_clean_k=file_clean_k.replace(to_replace ='\\]', value = '', regex = True)\n",
    "    return file_clean_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # My version of lemmatization based on the Coursera code/work I've done previously... \n",
    "\n",
    "# # Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, \n",
    "# # normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma.\n",
    "# # tl;dr put the verbs in their \"stem\" form. \n",
    "\n",
    "# def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "#     \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "#     texts_out = []\n",
    "#     for sent in texts:\n",
    "#         doc = nlp(\" \".join(sent)) \n",
    "#         texts_out.append(\" \".join([token.lemma_ if token.lemma_ not in ['-PRON-'] else '' for token in doc if token.pos_ in allowed_postags]))\n",
    "#     return texts_out\n",
    "\n",
    "# # Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "\n",
    "# nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# df_clean = wordLemmatizer(df['Word tokenize'][0:10]) \n",
    "# FOR THE WHOLE DATASET: \n",
    "df_clean = wordLemmatizer(df['Word tokenize'])\n",
    "df_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean=df_clean.replace(to_replace =\"\\[.\", value = '', regex = True)\n",
    "df_clean=df_clean.replace(to_replace =\"'\", value = '', regex = True)\n",
    "df_clean=df_clean.replace(to_replace =\" \", value = '', regex = True)\n",
    "df_clean=df_clean.replace(to_replace ='\\]', value = '', regex = True)\n",
    "df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do lemmatization keeping only Noun, Adjective, Verb, Adverb\n",
    "# This is using my version. \n",
    "# df_clean = lemmatization(df['Word tokenize'], allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "# print(df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add Lemmatized words to the dataframe\n",
    "## Insert New column in df to stored the Clean Keyword\n",
    "df.insert(loc=4, column='Clean_Keyword', value=df_clean['Keyword_final'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.drop(['Word tokenize','Clean_Keyword'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(\"df_twitch.csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Clean_Keyword[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade tensorflow-gpu\n",
    "#Install TF-Hub.\n",
    "!pip install tensorflow-hub\n",
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yay more imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, string\n",
    "import os \n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics.pairwise import linear_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Version: \", tf.__version__)\n",
    "print(\"Eager mode: \", tf.executing_eagerly())\n",
    "print(\"Hub version: \", hub.__version__)\n",
    "print(\"GPU is\", \"available\" if tf.config.experimental.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, using TF-IDF to create a document search tool. TF-IDF is the process of calculating the weight of each word (signifying the importance of the word in the corpus/document). The algorithm is used mainly for retrieving information and text mining.\n",
    "\n",
    "TF (Term Frequency) is how many times a word appears in a document divided by the number of the words in the document.\n",
    "\n",
    "IDF (Inverse Data Frequency) is the log of the number of documents divided by the number of documents with the word W that we're interested in.\n",
    "\n",
    "TF-IDF is just these two numbers multiplied together - sklearn implements this feature for you in their library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import operator\n",
    "\n",
    "## Create Vocabulary\n",
    "vocabulary = set()\n",
    "\n",
    "for doc in df.Clean_Keyword:\n",
    "    vocabulary.update(doc.split(','))\n",
    "\n",
    "vocabulary = list(vocabulary)\n",
    "\n",
    "# Intializating the tfIdf model\n",
    "tfidf = TfidfVectorizer(vocabulary=vocabulary,dtype=np.float32)\n",
    "\n",
    "# Fit the TfIdf model\n",
    "tfidf.fit(df.Clean_Keyword)\n",
    "\n",
    "# Transform the TfIdf model\n",
    "tfidf_tran=tfidf.transform(df.Clean_Keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary[0:10] # for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = list(filter(None, vocabulary)) # just removing the empty '' in the list! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the trained TF-ID Model\n",
    "# I uploaded this to Colab and then ran this. \n",
    "with open('/Users/ronak.k.bhatia/Desktop/Data_STA/tfid.pkl','wb') as handle:\n",
    "    pickle.dump(tfidf_tran, handle)\n",
    "\n",
    "# Load the model! \n",
    "t = pickle.load(open('/Users/ronak.k.bhatia/Desktop/Data_STA/tfid.pkl','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I have to actually create a text file that contains all the words that I'm interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !touch twitch_chat_analysis.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opens the created text file and writes the vocab list to it! \n",
    "f=open('twitch_chat_analysis.txt','w')\n",
    "s1='\\n'.join(vocabulary)\n",
    "f.write(s1)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the vocabulary\n",
    "with open(\"/Users/ronak.k.bhatia/Desktop/topic-modeling-coursera-notes/twitch_chat_analysis.txt\", \"w\") as file:\n",
    "    file.write(str(vocabulary))\n",
    "\n",
    "### Loading the vocabulary \n",
    "with open(\"/Users/ronak.k.bhatia/Desktop/topic-modeling-coursera-notes/twitch_chat_analysis.txt\", \"r\") as file:\n",
    "    data2 = eval(file.readline())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the Google Universal Sentence Encoder's pretrained Model \n",
    "\n",
    "base_dir = \"/Users/ronak.k.bhatia/Desktop/Data_STA/\"\n",
    "\n",
    "# !mkdir /Users/ronak.k.bhatia/Desktop/Data_STA/GoogleUSE Model\n",
    "# !curl -L -o 4.tar.gz \"https://tfhub.dev/google/universal-sentence-encoder/4?tf-hub-format=compressed\" \n",
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
    "\n",
    "# module_path =\"/Users/ronak.k.bhatia/Desktop/Data_STA/GoogleUSE Model/USE_4\"\n",
    "\n",
    "%time model = hub.load(module_url)\n",
    "#print (\"module %s loaded\" % module_url)\n",
    "\n",
    "#Create function for using modeltraining\n",
    "def embed(input):\n",
    "    return model(input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running Training Process on Google USE Model \n",
    "ls =[]\n",
    "chunksize =1000\n",
    "le =len(df.message)\n",
    "for i in range(0,le,chunksize):\n",
    "    if(i+chunksize > le): \n",
    "        chunksize= le;\n",
    "        ls.append(chunksize)\n",
    "    else:\n",
    "        a =i+chunksize\n",
    "        ls.append(a)\n",
    "ls\n",
    "j=0\n",
    "\n",
    "print(os)\n",
    "\n",
    "for i in ls:\n",
    "    directory = \"/Users/ronak.k.bhatia/Desktop/topic-modeling-coursera-notes/Twitch-Data/\" + str(i)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    directory = \"/Users/ronak.k.bhatia/Desktop/topic-modeling-coursera-notes/Twitch-Data/\" + str(i)\n",
    "    print(j,i) \n",
    "    m=embed(df.message[j:i])\n",
    "    exported_m = tf.train.Checkpoint(v=tf.Variable(m))\n",
    "    exported_m.f = tf.function(\n",
    "    lambda  x: exported_m.v * x,\n",
    "    input_signature=[tf.TensorSpec(shape=None, dtype=tf.float32)])\n",
    "\n",
    "    tf.saved_model.save(exported_m,directory)\n",
    "    j = i\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch-wise, loading the model! \n",
    "for i in ls:\n",
    "    directory = \"/Users/ronak.k.bhatia/Desktop/topic-modeling-coursera-notes/Twitch-Data/\" + str(i)\n",
    "    if os.path.exists(directory):\n",
    "        print(directory)\n",
    "        imported_m = tf.saved_model.load(directory)\n",
    "        a= imported_m.v.numpy()\n",
    "        #print(a)\n",
    "        exec(f'load{i} = a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the array from the batchwise loaded model. \n",
    "con_a =np.concatenate((load1000, \n",
    "                       load2000,\n",
    "                       load3000,\n",
    "                       load4000,\n",
    "                       load5000,\n",
    "                       load6000,\n",
    "                       load7000,\n",
    "                       load8000,\n",
    "                       load9000,\n",
    "                       load10000,\n",
    "                       load11000,\n",
    "                       load12000,\n",
    "                       load13000,\n",
    "                       load14000,\n",
    "                       load15000,\n",
    "                       load16000,\n",
    "                       load17000,\n",
    "                       load18000,\n",
    "                       load19000,\n",
    "                       load20000,\n",
    "                       load21000,\n",
    "                       load22000,\n",
    "                       load23000,\n",
    "                       load24000,\n",
    "                       load25000,\n",
    "                       load25617,\n",
    "                      ))\n",
    "con_a.shape # makes sense because USE encoder outputs a 512 layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model\n",
    "Model_USE= embed(df.message[0:3000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the model\n",
    "exported = tf.train.Checkpoint(v=tf.Variable(Model_USE))\n",
    "exported.f = tf.function(\n",
    "    lambda  x: exported.v * x,\n",
    "    input_signature=[tf.TensorSpec(shape=None, dtype=tf.float32)])\n",
    "\n",
    "tf.saved_model.save(exported,\"/Users/ronak.k.bhatia/Desktop/topic-modeling-coursera-notes/Twitch-Data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Create function to get the top most similar documents by giving input keyword/sentence\n",
    "\n",
    "def SearchDocument(query):\n",
    "    q =[query]\n",
    "    # embed the query for calcluating the similarity\n",
    "    Q_Train =embed(q)\n",
    "    \n",
    "    #loadedmodel =imported_m.v.numpy()\n",
    "    # Calculate the Similarity\n",
    "    linear_similarities = linear_kernel(Q_Train, con_a).flatten() \n",
    "    #Sort top 10 index with similarity score\n",
    "    Top_index_doc = linear_similarities.argsort()[:-400:-1]\n",
    "    # sort by similarity score\n",
    "    linear_similarities.sort()\n",
    "    a = pd.DataFrame()\n",
    "    for i,index in enumerate(Top_index_doc):\n",
    "        a.loc[i,'index'] = str(index)\n",
    "        a.loc[i,'Message in Chat'] = df['message'][index] ## Read File name with index from File_data DF\n",
    "    for j,simScore in enumerate(linear_similarities[:-400:-1]):\n",
    "        a.loc[j,'Score'] = simScore\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Message in Chat</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23915</td>\n",
       "      <td>!subscribercount</td>\n",
       "      <td>0.477913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23686</td>\n",
       "      <td>@shoootermcgavin 2xl</td>\n",
       "      <td>0.473387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2505</td>\n",
       "      <td>MR.UNLIMITED SHIRT</td>\n",
       "      <td>0.462875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21218</td>\n",
       "      <td>PauseChamping</td>\n",
       "      <td>0.459622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3928</td>\n",
       "      <td>loudly_crying_face:</td>\n",
       "      <td>0.458672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>370</td>\n",
       "      <td>PauseChamp</td>\n",
       "      <td>0.391119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>3176</td>\n",
       "      <td>PauseChamp</td>\n",
       "      <td>0.391119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>21342</td>\n",
       "      <td>PauseChamp</td>\n",
       "      <td>0.391119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>21199</td>\n",
       "      <td>PauseChamp</td>\n",
       "      <td>0.391119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>15339</td>\n",
       "      <td>PauseChamp</td>\n",
       "      <td>0.391119</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>399 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index       Message in Chat     Score\n",
       "0    23915      !subscribercount  0.477913\n",
       "1    23686  @shoootermcgavin 2xl  0.473387\n",
       "2     2505    MR.UNLIMITED SHIRT  0.462875\n",
       "3    21218         PauseChamping  0.459622\n",
       "4     3928   loudly_crying_face:  0.458672\n",
       "..     ...                   ...       ...\n",
       "394    370            PauseChamp  0.391119\n",
       "395   3176            PauseChamp  0.391119\n",
       "396  21342            PauseChamp  0.391119\n",
       "397  21199            PauseChamp  0.391119\n",
       "398  15339            PauseChamp  0.391119\n",
       "\n",
       "[399 rows x 3 columns]"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SearchDocument('jacket')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Semantic_Text_Similarity_Exploration.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
